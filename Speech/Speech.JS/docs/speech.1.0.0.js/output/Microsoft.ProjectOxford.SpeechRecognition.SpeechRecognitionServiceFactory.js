Ext.data.JsonP.Microsoft_ProjectOxford_SpeechRecognition_SpeechRecognitionServiceFactory({"tagname":"class","name":"Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory","autodetected":{},"files":[{"filename":"speech.1.0.0.js","href":"speech.1.0.0.html#Microsoft-ProjectOxford-SpeechRecognition-SpeechRecognitionServiceFactory"}],"members":[{"name":"createDataClient","tagname":"method","owner":"Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory","id":"method-createDataClient","meta":{}},{"name":"createDataClientWithIntent","tagname":"method","owner":"Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory","id":"method-createDataClientWithIntent","meta":{}},{"name":"createMicrophoneClient","tagname":"method","owner":"Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory","id":"method-createMicrophoneClient","meta":{}},{"name":"createMicrophoneClientWithIntent","tagname":"method","owner":"Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory","id":"method-createMicrophoneClientWithIntent","meta":{}}],"alternateClassNames":[],"aliases":{},"id":"class-Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory","component":false,"superclasses":[],"subclasses":[],"mixedInto":[],"mixins":[],"parentMixins":[],"requires":[],"uses":[],"html":"<div><pre class=\"hierarchy\"><h4>Files</h4><div class='dependency'><a href='source/speech.1.0.0.html#Microsoft-ProjectOxford-SpeechRecognition-SpeechRecognitionServiceFactory' target='_blank'>speech.1.0.0.js</a></div></pre><div class='doc-contents'>\n</div><div class='members'><div class='members-section'><div class='definedBy'>Defined By</div><h3 class='members-title icon-method'>Methods</h3><div class='subsection'><div id='method-createDataClient' class='member first-child not-inherited'><a href='#' class='side expandable'><span>&nbsp;</span></a><div class='title'><div class='meta'><span class='defined-in' rel='Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory'>Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory</span><br/><a href='source/speech.1.0.0.html#Microsoft-ProjectOxford-SpeechRecognition-SpeechRecognitionServiceFactory-method-createDataClient' target='_blank' class='view-source'>view source</a></div><a href='#!/api/Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory-method-createDataClient' class='name expandable'>createDataClient</a>( <span class='pre'>speechRecognitionMode, language, primaryKey, secondaryKey</span> ) : <a href=\"#!/api/Microsoft.ProjectOxford.SpeechRecognition.DataRecognitionClient\" rel=\"Microsoft.ProjectOxford.SpeechRecognition.DataRecognitionClient\" class=\"docClass\">Microsoft.ProjectOxford.SpeechRecognition.DataRecognitionClient</a><span class=\"signature\"></span></div><div class='description'><div class='short'> ...</div><div class='long'>\n<h3 class=\"pa\">Parameters</h3><ul><li><span class='pre'>speechRecognitionMode</span> : <a href=\"#!/api/Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionMode\" rel=\"Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionMode\" class=\"docClass\">Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionMode</a><div class='sub-desc'><p>In ShortPhrase mode the client gets one final multiple n-best choice result and\n                             in LongDictation mode the client will receive multiple final results, based on\n                             where the server thinks sentence pauses are.</p>\n</div></li><li><span class='pre'>language</span> : string<div class='sub-desc'><p>The language of the speech being recognized.  Examples include:\n    <br>American English: \"en-us\"\n    <br>British English: \"en-gb\"\n    <br>German: \"de-de\"\n    <br>Spanish: \"es-es\"\n    <br>French: \"fr-fr\"\n    <br>Italian: \"it-it\"\n    <br>Mandarin: \"zh-cn\"</p>\n</div></li><li><span class='pre'>primaryKey</span> : string<div class='sub-desc'><p>The primary key.  It's a best practice that the application rotate keys periodically.\n                  Between rotations, you would disable the primary key, making the secondary key the\n                  default, giving you time to swap out the primary.</p>\n</div></li><li><span class='pre'>secondaryKey</span> : string<div class='sub-desc'><p>The secondary key.  Intended to be used when the primary key has been disabled.</p>\n</div></li></ul><h3 class='pa'>Returns</h3><ul><li><span class='pre'><a href=\"#!/api/Microsoft.ProjectOxford.SpeechRecognition.DataRecognitionClient\" rel=\"Microsoft.ProjectOxford.SpeechRecognition.DataRecognitionClient\" class=\"docClass\">Microsoft.ProjectOxford.SpeechRecognition.DataRecognitionClient</a></span><div class='sub-desc'>\n</div></li></ul></div></div></div><div id='method-createDataClientWithIntent' class='member  not-inherited'><a href='#' class='side expandable'><span>&nbsp;</span></a><div class='title'><div class='meta'><span class='defined-in' rel='Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory'>Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory</span><br/><a href='source/speech.1.0.0.html#Microsoft-ProjectOxford-SpeechRecognition-SpeechRecognitionServiceFactory-method-createDataClientWithIntent' target='_blank' class='view-source'>view source</a></div><a href='#!/api/Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory-method-createDataClientWithIntent' class='name expandable'>createDataClientWithIntent</a>( <span class='pre'>language, primaryKey, secondaryKey, luisAppId, luisSubscriptionId</span> ) : <a href=\"#!/api/Microsoft.ProjectOxford.SpeechRecognition.DataRecognitionClient\" rel=\"Microsoft.ProjectOxford.SpeechRecognition.DataRecognitionClient\" class=\"docClass\">Microsoft.ProjectOxford.SpeechRecognition.DataRecognitionClient</a><span class=\"signature\"></span></div><div class='description'><div class='short'>Create a DataRecognitionClientWithIntent with Acoustic Model Adaptation -- for speech recognition with data (for exam...</div><div class='long'><p>Create a DataRecognitionClientWithIntent with Acoustic Model Adaptation -- for speech recognition with data (for example from a file or audio source).\nThe data is broken up into buffers and each buffer is sent to the Speech Recognition Service.\nNo modification is done to the buffers, so the user can apply their own Silence Detection. Returns both text recognition\nresults as well as structured intent results in JSON form from the LUIS (see https://LUIS.ai) service.\nThe audio must be PCM, mono, 16-bit sample, with sample rate of 8000 Hz or 16000 Hz.</p>\n<h3 class=\"pa\">Parameters</h3><ul><li><span class='pre'>language</span> : string<div class='sub-desc'><p>The language of the speech being recognized.  Examples include:\n    <br>American English: \"en-us\"\n    <br>British English: \"en-gb\"\n    <br>German: \"de-de\"\n    <br>Spanish: \"es-es\"\n    <br>French: \"fr-fr\"\n    <br>Italian: \"it-it\"\n    <br>Mandarin: \"zh-cn\"</p>\n</div></li><li><span class='pre'>primaryKey</span> : string<div class='sub-desc'><p>The primary key.  It's a best practice that the application rotate keys periodically.\n                  Between rotations, you would disable the primary key, making the secondary key the\n                  default, giving you time to swap out the primary.</p>\n</div></li><li><span class='pre'>secondaryKey</span> : string<div class='sub-desc'><p>The secondary key.  Intended to be used when the primary key has been disabled.</p>\n</div></li><li><span class='pre'>luisAppId</span> : string<div class='sub-desc'><p>Once you have configured the LUIS service to create and publish an intent model (see https://LUIS.ai)\n                 you will be given an Application ID guid.  Use that GUID here.</p>\n</div></li><li><span class='pre'>luisSubscriptionId</span> : string<div class='sub-desc'><p>Once you create a LUIS account (see https://LUIS.ai) you will be given an Subscription ID.\n                          Use that secret here.</p>\n</div></li></ul><h3 class='pa'>Returns</h3><ul><li><span class='pre'><a href=\"#!/api/Microsoft.ProjectOxford.SpeechRecognition.DataRecognitionClient\" rel=\"Microsoft.ProjectOxford.SpeechRecognition.DataRecognitionClient\" class=\"docClass\">Microsoft.ProjectOxford.SpeechRecognition.DataRecognitionClient</a></span><div class='sub-desc'>\n</div></li></ul></div></div></div><div id='method-createMicrophoneClient' class='member  not-inherited'><a href='#' class='side expandable'><span>&nbsp;</span></a><div class='title'><div class='meta'><span class='defined-in' rel='Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory'>Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory</span><br/><a href='source/speech.1.0.0.html#Microsoft-ProjectOxford-SpeechRecognition-SpeechRecognitionServiceFactory-method-createMicrophoneClient' target='_blank' class='view-source'>view source</a></div><a href='#!/api/Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory-method-createMicrophoneClient' class='name expandable'>createMicrophoneClient</a>( <span class='pre'>speechRecognitionMode, language, primaryKey, secondaryKey</span> ) : <a href=\"#!/api/Microsoft.ProjectOxford.SpeechRecognition.MicrophoneRecognitionClient\" rel=\"Microsoft.ProjectOxford.SpeechRecognition.MicrophoneRecognitionClient\" class=\"docClass\">Microsoft.ProjectOxford.SpeechRecognition.MicrophoneRecognitionClient</a><span class=\"signature\"></span></div><div class='description'><div class='short'>Create a MicrophoneRecognitionClient with Acoustic Model Adaptation -- for speech recognition from the microphone. ...</div><div class='long'><p>Create a MicrophoneRecognitionClient with Acoustic Model Adaptation -- for speech recognition from the microphone.  The microphone is\nturned on and data from the microphone is sent to the Speech Recognition Service.  Returns only text\nrecognition results.</p>\n<h3 class=\"pa\">Parameters</h3><ul><li><span class='pre'>speechRecognitionMode</span> : <a href=\"#!/api/Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionMode\" rel=\"Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionMode\" class=\"docClass\">Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionMode</a><div class='sub-desc'><p>In ShortPhrase mode the client gets one final multiple n-best choice result and\n                             in LongDictation mode the client will receive multiple final results, based on\n                             where the server thinks sentence pauses are.</p>\n</div></li><li><span class='pre'>language</span> : string<div class='sub-desc'><p>The language of the speech being recognized.  Examples include:\n    <br>American English: \"en-us\"\n    <br>British English: \"en-gb\"\n    <br>German: \"de-de\"\n    <br>Spanish: \"es-es\"\n    <br>French: \"fr-fr\"\n    <br>Italian: \"it-it\"\n    <br>Mandarin: \"zh-cn\"</p>\n</div></li><li><span class='pre'>primaryKey</span> : string<div class='sub-desc'><p>The primary key.  It's a best practice that the application rotate keys periodically.\n                  Between rotations, you would disable the primary key, making the secondary key the\n                  default, giving you time to swap out the primary.</p>\n</div></li><li><span class='pre'>secondaryKey</span> : string<div class='sub-desc'><p>The secondary key.  Intended to be used when the primary key has been disabled.</p>\n</div></li></ul><h3 class='pa'>Returns</h3><ul><li><span class='pre'><a href=\"#!/api/Microsoft.ProjectOxford.SpeechRecognition.MicrophoneRecognitionClient\" rel=\"Microsoft.ProjectOxford.SpeechRecognition.MicrophoneRecognitionClient\" class=\"docClass\">Microsoft.ProjectOxford.SpeechRecognition.MicrophoneRecognitionClient</a></span><div class='sub-desc'>\n</div></li></ul></div></div></div><div id='method-createMicrophoneClientWithIntent' class='member  not-inherited'><a href='#' class='side expandable'><span>&nbsp;</span></a><div class='title'><div class='meta'><span class='defined-in' rel='Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory'>Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory</span><br/><a href='source/speech.1.0.0.html#Microsoft-ProjectOxford-SpeechRecognition-SpeechRecognitionServiceFactory-method-createMicrophoneClientWithIntent' target='_blank' class='view-source'>view source</a></div><a href='#!/api/Microsoft.ProjectOxford.SpeechRecognition.SpeechRecognitionServiceFactory-method-createMicrophoneClientWithIntent' class='name expandable'>createMicrophoneClientWithIntent</a>( <span class='pre'>language, primaryKey, secondaryKey, luisAppId, luisSubscriptionId</span> ) : <a href=\"#!/api/Microsoft.ProjectOxford.SpeechRecognition.MicrophoneRecognitionClient\" rel=\"Microsoft.ProjectOxford.SpeechRecognition.MicrophoneRecognitionClient\" class=\"docClass\">Microsoft.ProjectOxford.SpeechRecognition.MicrophoneRecognitionClient</a><span class=\"signature\"></span></div><div class='description'><div class='short'>Create a MicrophoneRecognitionClientWithIntent -- for speech recognition from the microphone. ...</div><div class='long'><p>Create a MicrophoneRecognitionClientWithIntent -- for speech recognition from the microphone.  The microphone is\nturned on and data from the microphone is sent to the Speech Recognition Service.  Returns both\ntext recognition results as well as structured intent results in JSON form from the LUIS (see https://LUIS.ai) service.</p>\n<h3 class=\"pa\">Parameters</h3><ul><li><span class='pre'>language</span> : string<div class='sub-desc'><p>The language of the speech being recognized.  Examples include:\n    <br>American English: \"en-us\"\n    <br>British English: \"en-gb\"\n    <br>German: \"de-de\"\n    <br>Spanish: \"es-es\"\n    <br>French: \"fr-fr\"\n    <br>Italian: \"it-it\"\n    <br>Mandarin: \"zh-cn\"</p>\n</div></li><li><span class='pre'>primaryKey</span> : string<div class='sub-desc'><p>The primary key.  It's a best practice that the application rotate keys periodically.\n                  Between rotations, you would disable the primary key, making the secondary key the\n                  default, giving you time to swap out the primary.</p>\n</div></li><li><span class='pre'>secondaryKey</span> : string<div class='sub-desc'><p>The secondary key.  Intended to be used when the primary key has been disabled.</p>\n</div></li><li><span class='pre'>luisAppId</span> : string<div class='sub-desc'><p>Once you have configured the LUIS service to create and publish an intent model (see https://LUIS.ai)\n                 you will be given an Application ID guid.  Use that GUID here.</p>\n</div></li><li><span class='pre'>luisSubscriptionId</span> : string<div class='sub-desc'><p>Once you create a LUIS account (see https://LUIS.ai) you will be given an Subscription ID.\n                          Use that secret here.</p>\n</div></li></ul><h3 class='pa'>Returns</h3><ul><li><span class='pre'><a href=\"#!/api/Microsoft.ProjectOxford.SpeechRecognition.MicrophoneRecognitionClient\" rel=\"Microsoft.ProjectOxford.SpeechRecognition.MicrophoneRecognitionClient\" class=\"docClass\">Microsoft.ProjectOxford.SpeechRecognition.MicrophoneRecognitionClient</a></span><div class='sub-desc'>\n</div></li></ul></div></div></div></div></div></div></div>","meta":{}});